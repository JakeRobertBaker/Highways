{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n",
    "import math\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_text = \"https://webtris.highwaysengland.co.uk/api/v1/sites\"\n",
    "with urllib.request.urlopen(url_text) as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    \n",
    "sites = data['sites']\n",
    "sites[0]\n",
    "\n",
    "def get_quality_area(sites,\n",
    "                     max_lat,\n",
    "                     max_long,\n",
    "                     min_lat,\n",
    "                     min_long,\n",
    "                     start_date='01062021',\n",
    "                     end_date = '15062022',\n",
    "                     quality_threshold = 90):\n",
    "    \n",
    "    # Convert sites query into df and filter onto our area\n",
    "    sites_df = pd.DataFrame(data = sites)\n",
    "    area_sites_df = sites_df.loc[(min_long < sites_df.Longitude) & (sites_df.Longitude < max_long)\n",
    "                                & (min_lat < sites_df.Latitude) & (sites_df.Latitude < max_lat)]\n",
    "    area_sites_df = area_sites_df.reset_index(drop=True)\n",
    "    area_ids = list(area_sites_df.Id)\n",
    "    \n",
    "    # Next filter onto sites with good quality data:\n",
    "    quality_responces = []\n",
    "    for site_id in tqdm(area_ids):\n",
    "        url_text = f\"https://webtris.highwaysengland.co.uk/api/v1/quality/overall?sites={site_id}&start_date={start_date}&end_date={end_date}\"\n",
    "        with urllib.request.urlopen(url_text) as url:\n",
    "            responce = json.loads(url.read().decode())\n",
    "        quality_responces.append(responce)\n",
    "        \n",
    "    # We only want sites with quality greater than threshold\n",
    "    good_quality_ids = []\n",
    "    for responce in quality_responces:\n",
    "        if responce['data_quality'] >= quality_threshold:\n",
    "            good_quality_ids.append(responce['sites'])\n",
    "\n",
    "    quality_area_sites_df = area_sites_df.loc[area_sites_df.Id.isin(good_quality_ids)]\n",
    "    quality_area_sites_df = quality_area_sites_df.reset_index(drop=True)\n",
    "    \n",
    "    return quality_area_sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331/331 [00:23<00:00, 14.25it/s]\n",
      "100%|██████████| 231/231 [00:48<00:00,  4.78it/s]\n",
      "100%|██████████| 71/71 [00:05<00:00, 13.97it/s]\n",
      "100%|██████████| 69/69 [00:04<00:00, 14.11it/s]\n",
      "100%|██████████| 102/102 [00:07<00:00, 13.91it/s]\n",
      "100%|██████████| 150/150 [00:10<00:00, 14.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Specify dates\n",
    "start_date='19032019'\n",
    "end_date = '08042022'\n",
    "# Specify the train area we are looking at\n",
    "max_lat = 52.50\n",
    "max_long = -1.67\n",
    "min_lat = 52.42\n",
    "min_long = -1.75\n",
    "birmingham_sites_df = get_quality_area(sites,\n",
    "                     max_lat,\n",
    "                     max_long,\n",
    "                     min_lat,\n",
    "                     min_long,\n",
    "                     start_date,\n",
    "                     end_date)\n",
    "\n",
    "# Specify the train area we are looking at\n",
    "max_lat = 53.51\n",
    "max_long = -2.31\n",
    "min_lat = 53.44 \n",
    "min_long = -2.39\n",
    "manc_sites_df = get_quality_area(sites,\n",
    "                     max_lat,\n",
    "                     max_long,\n",
    "                     min_lat,\n",
    "                     min_long,\n",
    "                     start_date,\n",
    "                     end_date)\n",
    "\n",
    "# Specify the train area we are looking at\n",
    "max_lat = 52.25\n",
    "max_long = 0.11\n",
    "min_lat = 52.19\n",
    "min_long = 0.02\n",
    "# Get the quality reports\n",
    "cam_sites_df = get_quality_area(sites,\n",
    "                     max_lat,\n",
    "                     max_long,\n",
    "                     min_lat,\n",
    "                     min_long,\n",
    "                     start_date,\n",
    "                     end_date)\n",
    "\n",
    "# Specify the train area we are looking at\n",
    "max_lat = 51.43\n",
    "max_long = -0.50\n",
    "min_lat = 51.38\n",
    "min_long = -0.57\n",
    "quality_threshold = 40\n",
    "# Get the quality reports\n",
    "thorpe_sites_df = get_quality_area(sites,\n",
    "                     max_lat,\n",
    "                     max_long,\n",
    "                     min_lat,\n",
    "                     min_long,\n",
    "                     start_date,\n",
    "                     end_date)\n",
    "\n",
    "# Specify the train area we are looking at\n",
    "max_lat = 51.72\n",
    "max_long = 0.15\n",
    "min_lat = 51.62\n",
    "min_long = 0.09\n",
    "quality_threshold = 40\n",
    "# Get the quality reports\n",
    "epping_sites_df = get_quality_area(sites,\n",
    "                     max_lat,\n",
    "                     max_long,\n",
    "                     min_lat,\n",
    "                     min_long,\n",
    "                     start_date,\n",
    "                     end_date)\n",
    "\n",
    "# Specify the train area we are looking at\n",
    "max_lat = 51.60\n",
    "max_long = -2.52\n",
    "min_lat = 51.52\n",
    "min_long = -2.59\n",
    "# Get the quality reports\n",
    "bristol_sites_df = get_quality_area(sites,\n",
    "                     max_lat,\n",
    "                     max_long,\n",
    "                     min_lat,\n",
    "                     min_long,\n",
    "                     start_date,\n",
    "                     end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "birmingham_sites_df.to_feather('high_quality_site_reports/birmingham_sites_df')\n",
    "manc_sites_df.to_feather('high_quality_site_reports/manc_sites_df')\n",
    "cam_sites_df.to_feather('high_quality_site_reports/cam_sites_df')\n",
    "thorpe_sites_df.to_feather('high_quality_site_reports/thorpe_sites_df')\n",
    "epping_sites_df.to_feather('high_quality_site_reports/epping_sites_df')\n",
    "bristol_sites_df.to_feather('high_quality_site_reports/bristol_sites_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_report_query_url(site_id, page_num, start_date = '15062021', end_date = '15062022'):\n",
    "    query_url = f\"https://webtris.highwaysengland.co.uk/api/v1/reports/Daily?sites={site_id}&start_date={start_date}&end_date={end_date}&page={page_num}&page_size=10000\"\n",
    "    return query_url\n",
    "\n",
    "\n",
    "# Function gets the report for a particular site_id\n",
    "def get_site_report(site_id, start_date='15062021', end_date='15062022'):\n",
    "    # Download page 1\n",
    "    report_url = daily_report_query_url(site_id, 1, start_date, end_date)\n",
    "    with urllib.request.urlopen(report_url) as url:\n",
    "        report_page = json.loads(url.read().decode())\n",
    "        \n",
    "    # Work out how many pages are required    \n",
    "    header = report_page['Header']\n",
    "    rows = report_page['Rows']\n",
    "    row_count = header['row_count']\n",
    "    total_pages = math.ceil(row_count / 10000)\n",
    "    # Make a dataframe of the rows so dar\n",
    "    report_df = pd.DataFrame(data = rows)\n",
    "    \n",
    "    for i in range(2, total_pages+1):\n",
    "        # Get page i of the report\n",
    "        report_url = daily_report_query_url(site_id, i, start_date, end_date)\n",
    "        with urllib.request.urlopen(report_url) as url:\n",
    "            report_page = json.loads(url.read().decode())\n",
    "        \n",
    "        rows = report_page['Rows']\n",
    "        current_page_df = pd.DataFrame(data = rows)\n",
    "        report_df = pd.concat([report_df, current_page_df], ignore_index=True)\n",
    "\n",
    "    return report_df, header\n",
    "\n",
    "# Function takes a dataframe of site_df and gets the reports\n",
    "def get_reports_from_sites_df(sites_df, start_date, end_date):\n",
    "    # Get the reports on the site\n",
    "    train_reports =  collections.defaultdict(str)\n",
    "    # Go through all the site ids and get reports\n",
    "    for site_id in tqdm(sites_df.Id):\n",
    "        report, header = get_site_report(site_id, start_date, end_date)\n",
    "        report['site_id'] = site_id\n",
    "        train_reports[site_id] = report\n",
    "        \n",
    "    # Combine reports into one df\n",
    "    report_df = pd.concat(list(train_reports.values()), ignore_index=True)\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [10:31<00:00, 24.30s/it]\n",
      "100%|██████████| 40/40 [16:07<00:00, 24.18s/it]\n",
      "100%|██████████| 7/7 [02:47<00:00, 23.95s/it]\n",
      "100%|██████████| 36/36 [14:29<00:00, 24.16s/it]\n",
      "100%|██████████| 15/15 [06:04<00:00, 24.31s/it]\n",
      "100%|██████████| 5/5 [01:59<00:00, 24.00s/it]\n"
     ]
    }
   ],
   "source": [
    "# Take the start and end dates as the maximum and minima of the datas + a month\n",
    "start_date='19032019'\n",
    "end_date = '08042022'\n",
    "\n",
    "birmingham_report_df = get_reports_from_sites_df(birmingham_sites_df, start_date, end_date)\n",
    "birmingham_report_df.to_feather('high_quality_traffic_reports/birmingham_report_df')\n",
    "\n",
    "manc_report_df = get_reports_from_sites_df(manc_sites_df, start_date, end_date)\n",
    "manc_report_df.to_feather('high_quality_traffic_reports/manc_report_df')\n",
    "\n",
    "cam_report_df = get_reports_from_sites_df(cam_sites_df, start_date, end_date)\n",
    "cam_report_df.to_feather('high_quality_traffic_reports/cam_report_df')\n",
    "\n",
    "thorpe_report_df = get_reports_from_sites_df(thorpe_sites_df, start_date, end_date)\n",
    "thorpe_report_df.to_feather('high_quality_traffic_reports/thorpe_report_df')\n",
    "\n",
    "epping_report_df = get_reports_from_sites_df(epping_sites_df, start_date, end_date)\n",
    "epping_report_df.to_feather('high_quality_traffic_reports/epping_report_df')\n",
    "\n",
    "bristol_report_df = get_reports_from_sites_df(bristol_sites_df, start_date, end_date)\n",
    "bristol_report_df.to_feather('high_quality_traffic_reports/bristol_report_df')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python sklearn",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
